\chapter{Scheduler events}
\label{ch:events_documentation}
A simple script can report the tracepoint location of every event that we are looking for.
\begin{codebash}
#!/bin/bash
# find_sched_events.sh
events=$(ls /sys/kernel/debug/tracing/events/sched | grep "sched_" | sort)
cd /path/to/linux-4.20.13
for i in $events
do
	grep -rin "trace_$i" >> ../events_output
done
\end{codebash}
By executing the script and then ordering by source file, we obtain the following output:

\begin{Verbatim}[xleftmargin=-0.5cm,fontsize=\footnotesize]
fs/exec.c:1698: trace_sched_process_exec(current, old_pid, bprm);

kernel/exit.c:1503: trace_sched_process_wait(wo->wo_pid);
kernel/exit.c:180: trace_sched_process_free(tsk);
kernel/exit.c:866: trace_sched_process_exit(tsk);

kernel/fork.c:2242: trace_sched_process_fork(current, p);

kernel/hung_task.c:113: trace_sched_process_hang(t);

kernel/kthread.c:543: trace_sched_kthread_stop(k);
kernel/kthread.c:554: trace_sched_kthread_stop_ret(ret);

kernel/sched/core.c:1171: trace_sched_migrate_task(p, new_cpu);
kernel/sched/core.c:1295: trace_sched_swap_numa(cur, arg.src_cpu, p, arg.dst_cpu);
kernel/sched/core.c:1358: trace_sched_wait_task(p);
kernel/sched/core.c:1659: trace_sched_wakeup(p);
kernel/sched/core.c:1798: trace_sched_wake_idle_without_ipi(cpu);
kernel/sched/core.c:1813: trace_sched_wake_idle_without_ipi(cpu);
kernel/sched/core.c:473: trace_sched_wake_idle_without_ipi(cpu);
kernel/sched/core.c:545: trace_sched_wake_idle_without_ipi(cpu);
kernel/sched/core.c:1970: trace_sched_waking(p);
kernel/sched/core.c:2100: trace_sched_waking(p);
kernel/sched/core.c:2425: trace_sched_wakeup_new(p);
kernel/sched/core.c:3469: trace_sched_switch(preempt, prev, next);
kernel/sched/core.c:3797: trace_sched_pi_setprio(p, pi_task);
kernel/sched/core.c:5485: trace_sched_move_numa(p, curr_cpu, target_cpu);

kernel/sched/fair.c:1836: trace_sched_stick_numa(p, env.src_cpu, env.best_cpu);
kernel/sched/fair.c:1844: trace_sched_stick_numa(p, env.src_cpu, task_cpu(env.best_task));
kernel/sched/fair.c:833: trace_sched_stat_runtime(curtask, delta_exec, curr->vruntime);
kernel/sched/fair.c:886: trace_sched_stat_wait(p, delta);
kernel/sched/fair.c:925: trace_sched_stat_sleep(tsk, delta);
kernel/sched/fair.c:944: trace_sched_stat_iowait(tsk, delta);
kernel/sched/fair.c:947: trace_sched_stat_blocked(tsk, delta);
\end{Verbatim}
There are 24 events and 29 tracepoints where they are used. The output shows the tracepoints with their respective source file and line. Thanks to this output and the developer's comments, it is now easier to understand the purpose of each event. For each event there are listed: 
\begin{enumerate}
    \item Its parameters
    \item The information printed in the trace output
    \item Usages in the code (the tracepoints)
    \item A brief explanation
\end{enumerate}

\section{Events in \texttt{core.c}} 
\paragraph{\texttt{trace\_sched\_migrate\_task}}
Tracepoint for a task being migrated from \verb|orig_cpu| to \verb|dest_cpu|.

\textbf{Parameters}
\begin{itemize}
    \item \verb|struct task_struct *p| -- Migrated task
    \item \verb|int dest_cpu|
\end{itemize}

\textbf{Trace output}
\begin{itemize}
    \item \verb|char *comm| -- Command string that spawned the process
    \item \verb|pid_t pid|
    \item \verb|int prio|
    \item \verb|int orig_cpu, dest_cpu|
\end{itemize}

\textbf{Usages}
\begin{code}
// ./kernel/sched/core.c:1171
void set_task_cpu(struct task_struct *p, unsigned int new_cpu){
#ifdef CONFIG_SCHED_DEBUG // Debug messages
	/*
	 * We should never call set_task_cpu() on a blocked task,
	 * ttwu() will sort out the placement.
	 */
	WARN_ON_ONCE(p->state != TASK_RUNNING && p->state != TASK_WAKING &&
			!p->on_rq);
        // ...
	WARN_ON_ONCE(p->state == TASK_RUNNING &&
		     p->sched_class == &fair_sched_class &&
		     (p->on_rq && !task_on_rq_migrating(p)));

        // ...
	/*
	 * Clearly, migrating tasks to offline CPUs is a fairly daft thing.
	 */
	WARN_ON_ONCE(!cpu_online(new_cpu));
#endif 

	trace_sched_migrate_task(p, new_cpu); // Tracepoint

	if (task_cpu(p) != new_cpu) {
		if (p->sched_class->migrate_task_rq)
			p->sched_class->migrate_task_rq(p, new_cpu);
		p->se.nr_migrations++;
		rseq_migrate(p);
		perf_event_task_migrate(p);
	}

	__set_task_cpu(p, new_cpu);
}
\end{code}
This function migrates a task to \verb|new_cpu|, essentially changing its runqueue (in the subroutine \verb|__set_task_cpu()|).
  
\paragraph{\texttt{trace\_sched\_swap\_numa}}
Triggered when two task in two different CPUs are swapped. Takes NUMA into account.

\textbf{Parameters}
\begin{itemize}
    \item \verb|struct task_struct *src_tsk|
    \item \verb|struct task_struct *dst_tsk|
    \item \verb|int src_cpu|
    \item \verb|int dst_cpu|    
\end{itemize}

\textbf{Trace output}
\begin{itemize}
    \item \verb|pid_t src_pid, src_tgid, src_ngid| -- pid, thread group id, and NUMA group id
    \item \verb|int src_cpu, src_nid| -- nid is the NUMA node id of the cpu \verb|src_cpu|
    \item \verb|pid_t dst_pid, dst_tgid, dst_ngid|
    \item \verb|int dst_cpu, dst_nid|
\end{itemize}

\textbf{Usages}
\begin{code}
// ./kernel/sched/core.c:1295
struct migration_swap_arg {
	struct task_struct *src_task, *dst_task;
	int src_cpu, dst_cpu;
};
/*
 * Cross migrate two tasks
 */
int migrate_swap(struct task_struct *cur, struct task_struct *p,
		int target_cpu, int curr_cpu){
	struct migration_swap_arg arg;
	int ret = -EINVAL;

	arg = (struct migration_swap_arg){
		.src_task = cur,
		.src_cpu = curr_cpu,
		.dst_task = p,
		.dst_cpu = target_cpu,
	};

	if (arg.src_cpu == arg.dst_cpu)
		goto out;

	/*
	 * These three tests are all lockless; this is OK since all of them
	 * will be re-checked with proper locks held further down the line.
	 */
	if (!cpu_active(arg.src_cpu) || !cpu_active(arg.dst_cpu))
		goto out;

	if (!cpumask_test_cpu(arg.dst_cpu, &arg.src_task->cpus_allowed))
		goto out;

	if (!cpumask_test_cpu(arg.src_cpu, &arg.dst_task->cpus_allowed))
		goto out;

	trace_sched_swap_numa(cur, arg.src_cpu, p, arg.dst_cpu); // Tracepoint
	ret = stop_two_cpus(arg.dst_cpu, arg.src_cpu, migrate_swap_stop, &arg);

out:
	return ret;
}
\end{code}
\verb|stop_two_cpus()| stops the two CPUs and runs \verb|migrate_swap_stop| with arguments \verb|arg| on the first CPU, returns when both are done.

\paragraph{\texttt{trace\_sched\_wait\_task}}
Tracepoint for waiting on task to unschedule.

\textbf{Parameters}
\begin{itemize}
    \item \verb|struct task_struct *p| -- Waited task
\end{itemize}

\textbf{Trace output (\texttt{sched\_process\_template})}
\begin{itemize}
    \item \verb|char *comm|
    \item \verb|pid_t pid|
    \item \verb|int prio|
\end{itemize}

\textbf{Usages}
\begin{code}
// ./kernel/sched/core.c:1358
/*
 * wait_task_inactive - wait for a thread to unschedule.
 *
 * If @match_state is nonzero, it's the @p->state value just checked and
 * not expected to change.  If it changes, i.e. @p might have woken up,
 * then return zero.  When we succeed in waiting for @p to be off its CPU,
 * we return a positive number (its total switch count).  If a second call
 * a short while later returns the same number, the caller can be sure that
 * @p has remained unscheduled the whole time.
 *
 * The caller must ensure that the task *will* unschedule sometime soon,
 * else this function might spin for a *long* time. This function can't
 * be called with interrupts off, or it may introduce deadlock with
 * smp_call_function() if an IPI is sent by the same process we are
 * waiting to become inactive.
 */
unsigned long wait_task_inactive(struct task_struct *p, long match_state){
	int running, queued;
	struct rq_flags rf;
	unsigned long ncsw;
	struct rq *rq;

	for (;;) {
		/*
		 * We do the initial early heuristics without holding
		 * any task-queue locks at all. We'll only try to get
		 * the runqueue lock when things look like they will
		 * work out!
		 */
		rq = task_rq(p);

		/*
		 * If the task is actively running on another CPU
		 * still, just relax and busy-wait without holding
		 * any locks.
		 *
		 * NOTE! Since we don't hold any locks, it's not
		 * even sure that "rq" stays as the right runqueue!
		 * But we don't care, since "task_running()" will
		 * return false if the runqueue has changed and p
		 * is actually now running somewhere else!
		 */
		while (task_running(rq, p)) {
			if (match_state && unlikely(p->state != match_state))
				return 0;
			cpu_relax();
		}

		/*
		 * Ok, time to look more closely! We need the rq
		 * lock now, to be *sure*. If we're wrong, we'll
		 * just go back and repeat.
		 */
		rq = task_rq_lock(p, &rf);
		trace_sched_wait_task(p); // Tracepoint
		running = task_running(rq, p);
		queued = task_on_rq_queued(p);
		ncsw = 0;
		if (!match_state || p->state == match_state)
			ncsw = p->nvcsw | LONG_MIN; /* sets MSB */
		task_rq_unlock(rq, p, &rf);

		/*
		 * If it changed from the expected state, bail out now.
		 */
		if (unlikely(!ncsw))
			break;

		/*
		 * Was it really running after all now that we
		 * checked with the proper locks actually held?
		 *
		 * Oops. Go back and try again..
		 */
		if (unlikely(running)) {
			cpu_relax();
			continue;
		}

		/*
		 * It's not enough that it's not actively running,
		 * it must be off the runqueue _entirely_, and not
		 * preempted!
		 *
		 * So if it was still runnable (but just not actively
		 * running right now), it's preempted, and we should
		 * yield - it could be a while.
		 */
		if (unlikely(queued)) {
			ktime_t to = NSEC_PER_SEC / HZ;

			set_current_state(TASK_UNINTERRUPTIBLE);
			schedule_hrtimeout(&to, HRTIMER_MODE_REL);
			continue;
		}

		/*
		 * Ahh, all good. It wasn't running, and it wasn't
		 * runnable, which means that it will never become
		 * running in the future either. We're all done!
		 */
		break;
	}

	return ncsw;
}
\end{code}
This function essentially busywaits until the task gets unscheduled. Note that \verb|nvcsw| is the context switch count, which is returned if the process did unschedule. The event is triggered once the lock on the runqueue is held: this doesn't mean that the process did actually unschedule at that point, many other checks still must be done.

\paragraph{\texttt{trace\_sched\_wakeup}}
Tracepoint called when the task is actually woken up\\ (\verb|p->state == TASK_RUNNING|). When any task wakes up, wakeup preemption is checked: at that point in the code, this event is triggered. It it not always called from the waking context.

\textbf{Parameters}
\begin{itemize}
    \item \verb|struct task_struct *p| -- Woken task
\end{itemize}

\textbf{Trace output (\texttt{sched\_wakeup\_template})}
\begin{itemize}
    \item \verb|char *comm|
    \item \verb|pid_t pid|
    \item \verb|int prio|
    \item \verb|int success|
    \item \verb|int target_cpu|
\end{itemize}

\textbf{Usages}
\begin{code}
// ./kernel/sched/core.c:1659
/*
 * Mark the task runnable and perform wakeup-preemption.
 */
static void ttwu_do_wakeup(struct rq *rq, struct task_struct *p, int wake_flags, struct rq_flags *rf){
	check_preempt_curr(rq, p, wake_flags); // Checks if it should be preempted, if yes sets TIF_NEED_RESCHED
	p->state = TASK_RUNNING;
	trace_sched_wakeup(p); // Tracepoint

#ifdef CONFIG_SMP
	if (p->sched_class->task_woken) {
		/*
		 * Our task @p is fully woken up and running; so its safe to
		 * drop the rq->lock, hereafter rq is only used for statistics.
		 */
		rq_unpin_lock(rq, rf);
		p->sched_class->task_woken(rq, p);
		rq_repin_lock(rq, rf);
	}

	if (rq->idle_stamp) {
		u64 delta = rq_clock(rq) - rq->idle_stamp;
		u64 max = 2*rq->max_idle_balance_cost;

		update_avg(&rq->avg_idle, delta);

		if (rq->avg_idle > max)
			rq->avg_idle = max;

		rq->idle_stamp = 0;
	}
#endif
}
\end{code}
``\verb|ttwu|'' means ``try to wake up''. In this point in the code, the task is marked as runnable and then wakeup preemption is performed. ``Wakeup preemption'' refers to the kind of preemption discussed in Section \ref{sec:runqueue_wakeup_reschedule}, where this event was also mentioned (Subsection ``A task is woken up'').
  
\paragraph{\texttt{trace\_sched\_wake\_idle\_without\_ipi}}
Tracepoint for waking a polling cpu without an IPI. An IPI is a \textit{inter processor interrupt}, an interrupt sent from one processor to another (which means we're on  a SMP system). IPIs are used, for example, when the system is being shut down by one processor, and all the other processors need to be notified. In our case, sending an IPI is needed if the task to reschedule is on a different CPU than the one we're running. If this fails, is is because that CPU is polling for \verb|TIF_NEED_RESCHED| (\verb|TIF_POLLING_NRFLAG| is set, \verb|NRFLAG| = Need Rescheduling Flag). In this case, we wake that CPU without using an IPI, and the event is triggered.

\textbf{Parameters}
\begin{itemize}
    \item \verb|int cpu| -- Woken CPU
\end{itemize}

\textbf{Trace output}
\begin{itemize}
    \item \verb|int cpu|
\end{itemize}

\textbf{Usages}
\label{sec:resched_curr}
\begin{code}
// ./kernel/sched/core.c:473:
/*
 * resched_curr - mark rq's current task 'to be rescheduled now'.
 *
 * On UP this means the setting of the need_resched flag, on SMP it
 * might also involve a cross-CPU call to trigger the scheduler on
 * the target CPU.
 */
void resched_curr(struct rq *rq){
	struct task_struct *curr = rq->curr;
	int cpu;

	lockdep_assert_held(&rq->lock);

	if (test_tsk_need_resched(curr)) // The flag is already set, we're done
		return;

	cpu = cpu_of(rq);

	if (cpu == smp_processor_id()) { // We are running on the same CPU as the task we reschedule
		set_tsk_need_resched(curr);
		set_preempt_need_resched();
		return;
	}

	if (set_nr_and_not_polling(curr)) // Sets NEED_RESCHEDULE
		smp_send_reschedule(cpu); // Sends IPI
	else
		trace_sched_wake_idle_without_ipi(cpu); // Tracepoint
}
\end{code}
This is the \verb|resched_curr| function mentioned in Section \ref{trace:wake_idle_without_ipi}. The other 3 tracepoints (lines 545/1798/1813) are analogous to this one.

\paragraph{\texttt{trace\_sched\_waking}}
Tracepoint called when waking a task; this tracepoint is guaranteed to be called from the waking context. This means that the task will be in the \verb|TASK_WAKING| state.
This event was mentioned in Section \ref{trace:sched_waking} ``A task is woken up''.

\textbf{Parameters}
\begin{itemize}
    \item \verb|struct task_struct *p| -- Waking task
\end{itemize}

\textbf{Trace output (\texttt{sched\_wakeup\_template})}
\begin{itemize}
    \item \verb|char *comm|
    \item \verb|pid_t pid|
    \item \verb|int prio|
    \item \verb|int success|
    \item \verb|int target_cpu|
\end{itemize}

\textbf{Usages}
\begin{code}
// ./kernel/sched/core.c:1970
/**
 * try_to_wake_up - wake up a thread
 * @p: the thread to be awakened
 * @state: the mask of task states that can be woken
 * @wake_flags: wake modifier flags (WF_*)
 *
 * If (@state & @p->state) @p->state = TASK_RUNNING.
 *
 * If the task was not queued/runnable, also place it back on a runqueue.
 *
 * Atomic against schedule() which would dequeue a task, also see
 * set_current_state().
 *
 * This function executes a full memory barrier before accessing the task
 * state; see set_current_state().
 *
 * Return: %true if @p->state changes (an actual wakeup was done),
 *	   %false otherwise.
 */
static int
try_to_wake_up(struct task_struct *p, unsigned int state, int wake_flags){
	unsigned long flags;
	int cpu, success = 0;

	/*
	 * If we are going to wake up a thread waiting for CONDITION we
	 * need to ensure that CONDITION=1 done by the caller can not be
	 * reordered with p->state check below. This pairs with mb() in
	 * set_current_state() the waiting thread does.
	 */
	raw_spin_lock_irqsave(&p->pi_lock, flags);
	smp_mb__after_spinlock();
	if (!(p->state & state))
		goto out;

	trace_sched_waking(p); // Tracepoint

	/* We're going to change ->state: */
	success = 1;
	cpu = task_cpu(p);
        // ...
	smp_rmb();
	if (p->on_rq && ttwu_remote(p, wake_flags))
		goto stat;
        // ...
	ttwu_queue(p, cpu, wake_flags);
stat:
	ttwu_stat(p, cpu, wake_flags);
out:
	raw_spin_unlock_irqrestore(&p->pi_lock, flags);

	return success;
}
\end{code}
\begin{code}
// ./kernel/sched/core.c:2100
/**
 * try_to_wake_up_local - try to wake up a local task with rq lock held
 * @p: the thread to be awakened
 * @rf: request-queue flags for pinning
 *
 * Put @p on the run-queue if it's not already there. The caller must
 * ensure that this_rq() is locked, @p is bound to this_rq() and not
 * the current task.
 */
static void try_to_wake_up_local(struct task_struct *p, struct rq_flags *rf){
	struct rq *rq = task_rq(p);

	if (WARN_ON_ONCE(rq != this_rq()) ||
	    WARN_ON_ONCE(p == current))
		return;

	lockdep_assert_held(&rq->lock);

	if (!raw_spin_trylock(&p->pi_lock)) {
		/*
		 * This is OK, because current is on_cpu, which avoids it being
		 * picked for load-balance and preemption/IRQs are still
		 * disabled avoiding further scheduler activity on it and we've
		 * not yet picked a replacement task.
		 */
		rq_unlock(rq, rf);
		raw_spin_lock(&p->pi_lock);
		rq_relock(rq, rf);
	}

	if (!(p->state & TASK_NORMAL))
		goto out;

	trace_sched_waking(p); // Tracepoint

	if (!task_on_rq_queued(p)) {
		if (p->in_iowait) {
			delayacct_blkio_end(p);
			atomic_dec(&rq->nr_iowait);
		}
		ttwu_activate(rq, p, ENQUEUE_WAKEUP | ENQUEUE_NOCLOCK);
	}

	ttwu_do_wakeup(rq, p, 0, rf);
	ttwu_stat(p, smp_processor_id(), 0);
out:
	raw_spin_unlock(&p->pi_lock);
}
\end{code}

\paragraph{\texttt{trace\_sched\_wakeup\_new}}
Tracepoint for waking up a new task.

\textbf{Parameters}
\begin{itemize}
    \item \verb|struct task_struct *p| -- New task that is woken up for the first time
\end{itemize}

\textbf{Trace output (\texttt{sched\_wakeup\_template})}
\begin{itemize}
    \item \verb|char *comm|
    \item \verb|pid_t pid|
    \item \verb|int prio|
    \item \verb|int success|
    \item \verb|int target_cpu|
\end{itemize}

\textbf{Usages}
\begin{code}
// ./kernel/sched/core.c:2425
/*
 * wake_up_new_task - wake up a newly created task for the first time.
 *
 * This function will do some initial scheduler statistics housekeeping
 * that must be done for every newly created context, then puts the task
 * on the runqueue and wakes it.
 */
void wake_up_new_task(struct task_struct *p){
	struct rq_flags rf;
	struct rq *rq;

	raw_spin_lock_irqsave(&p->pi_lock, rf.flags);
	p->state = TASK_RUNNING;
#ifdef CONFIG_SMP
	/*
	 * Fork balancing, do it here and not earlier because:
	 *  - cpus_allowed can change in the fork path
	 *  - any previously selected CPU might disappear through hotplug
	 *
	 * Use __set_task_cpu() to avoid calling sched_class::migrate_task_rq,
	 * as we're not fully set-up yet.
	 */
	p->recent_used_cpu = task_cpu(p);
	__set_task_cpu(p, select_task_rq(p, task_cpu(p), SD_BALANCE_FORK, 0));
#endif
	rq = __task_rq_lock(p, &rf);
	update_rq_clock(rq);
	post_init_entity_util_avg(&p->se);

	activate_task(rq, p, ENQUEUE_NOCLOCK);
	p->on_rq = TASK_ON_RQ_QUEUED;
	trace_sched_wakeup_new(p); // Tracepoint
	check_preempt_curr(rq, p, WF_FORK);
#ifdef CONFIG_SMP
	if (p->sched_class->task_woken) {
		/*
		 * Nothing relies on rq->lock after this, so its fine to
		 * drop it.
		 */
		rq_unpin_lock(rq, &rf);
		p->sched_class->task_woken(rq, p);
		rq_repin_lock(rq, &rf);
	}
#endif
	task_rq_unlock(rq, p, &rf);
}
\end{code}
This function performs wakeup preemption for newly created tasks; it is called the first time a process wakes up. This event was mentioned in Section \ref{trace:sched_wakeup_new} ``A new process is created''.
   
\paragraph{\texttt{trace\_sched\_switch}}
Tracepoint for context switches, performed by the scheduler.

\textbf{Parameters}
\begin{itemize}
    \item \verb|bool preempt| -- Is preemption enabled while performing the schedule?
    \item \verb|struct task_struct *prev| -- Preempted task
    \item \verb|struct task_struct *next| -- Newly scheduled task
\end{itemize}

\textbf{Trace output}
\begin{itemize}
    \item \verb|char *prev_comm|
    \item \verb|pid_t prev_pid|
    \item \verb|int prev_prio|
    \item \verb|long prev_state|
    \item \verb|char *next_comm|
    \item \verb|pid_t next_pid|
    \item \verb|int next_prio|
\end{itemize}

\textbf{Usages}

The call chain is:
\begin{code}
schedule(){
    __schedule(){
        context_switch();
    }
}
\end{code}
The tracepoint is in \verb|__schedule()|, just before calling \verb|context_switch()|.
\begin{code}
asmlinkage __visible void __sched schedule(void) {
	struct task_struct *tsk = current;

	sched_submit_work(tsk);
	do {
		preempt_disable(); // Preemption disabled while scheduling
		__schedule(false);
		sched_preempt_enable_no_resched();
	} while (need_resched());
}
\end{code}
\begin{code}
// ./kernel/sched/core.c:3469
static void __sched notrace __schedule(bool preempt) {
	struct task_struct *prev, *next;
	unsigned long *switch_count;
	struct rq_flags rf;
	struct rq *rq;
	int cpu;

	cpu = smp_processor_id();
	rq = cpu_rq(cpu);
	prev = rq->curr;
        // ...
	next = pick_next_task(rq, prev, &rf);
	clear_tsk_need_resched(prev);
	clear_preempt_need_resched();

	if (likely(prev != next)) {
		rq->nr_switches++;
		rq->curr = next;
                // ...
		++*switch_count;

		trace_sched_switch(preempt, prev, next); // Tracepoint

		/* Also unlocks the rq: */
		rq = context_switch(rq, prev, next, &rf);
	} else {
		rq->clock_update_flags &= ~(RQCF_ACT_SKIP|RQCF_REQ_SKIP);
		rq_unlock_irq(rq, &rf);
	}

	balance_callback(rq);
}
\end{code}
\verb|context_switch()| is called when scheduling in order to complete the task switch. 
Internally, \verb|context_switch| performs two operations:
\begin{itemize}
    \item Switches the virtual memory areas of the two processes (\verb|mm_struct|)
    \item Restores the hardware context (the state of the registers) of the new process, and saves the old one's hardware context in its \verb|task_struct|.
\end{itemize}
If a task's \verb|task_struct| has \verb|mm| at \verb|NULL|, then it is a kernel thread.\label{sec:context_switch}
\begin{code}
/*
 * context_switch - switch to the new MM and the new thread's register state.
 */
static __always_inline struct rq *
context_switch(struct rq *rq, struct task_struct *prev,
	       struct task_struct *next, struct rq_flags *rf) {
	struct mm_struct *mm, *oldmm;

	prepare_task_switch(rq, prev, next);

	mm = next->mm;
	oldmm = prev->active_mm;
        // ...
	arch_start_context_switch(prev); // Architecture dependent

	/*
	 * If mm is non-NULL, we pass through switch_mm(). If mm is
	 * NULL, we will pass through mmdrop() in finish_task_switch().
	 * Both of these contain the full memory barrier required by
	 * membarrier after storing to rq->curr, before returning to
	 * user-space.
	 */
	if (!mm) {
		next->active_mm = oldmm;
		mmgrab(oldmm);
		enter_lazy_tlb(oldmm, next);
	} else
		switch_mm_irqs_off(oldmm, mm, next);

	if (!prev->mm) {
		prev->active_mm = NULL;
		rq->prev_mm = oldmm;
	}

	rq->clock_update_flags &= ~(RQCF_ACT_SKIP|RQCF_REQ_SKIP);

	prepare_lock_switch(rq, next, rf);

	/* Here we just switch the register state and the stack. */
	switch_to(prev, next, prev);
	barrier();

	return finish_task_switch(prev);
}
\end{code}

\paragraph{\texttt{trace\_sched\_pi\_setprio}}
Tracepoint for showing priority inheritance (\verb|pi|) modifying a tasks priority. Priority inheritance is a technique to avoid a scenario called priority inversion, in which a high priority task is indirectly preempted by a lower priority task effectively inverting the relative priorities of the two tasks (it happens when the tasks share a resource). With priority inheritance, when a high priority task has to wait for some resource shared with an executing low priority task, the low priority task is temporarily assigned the priority of the highest waiting priority task for the duration of its own use of the shared resource.

\textbf{Parameters}
\begin{itemize}
    \item \verb|struct task_struct *tsk| -- Task to boost
    \item \verb|struct task_struct *pi_task| -- Donor task
\end{itemize}

\textbf{Trace output}
\begin{itemize}
    \item \verb|char *comm|
    \item \verb|pid_t pid|
    \item \verb|int oldprio| -- \verb|tsk|'s old priority
    \item \verb|int newprio| -- \verb|tsk|'s boosted priority
\end{itemize}

\textbf{Usages}
\begin{code}
// ./kernel/sched/core.c:3797
/*
 * rt_mutex_setprio - set the current priority of a task
 * @p: task to boost
 * @pi_task: donor task
 *
 * This function changes the 'effective' priority of a task. It does
 * not touch ->normal_prio like __setscheduler().
 *
 * Used by the rt_mutex code to implement priority inheritance
 * logic. Call site only calls if the priority of the task changed.
 */
void rt_mutex_setprio(struct task_struct *p, struct task_struct *pi_task){
	int prio, oldprio, queued, running, queue_flag =
		DEQUEUE_SAVE | DEQUEUE_MOVE | DEQUEUE_NOCLOCK;
	const struct sched_class *prev_class;
	struct rq_flags rf;
	struct rq *rq;

	/* XXX used to be waiter->prio, not waiter->task->prio */
	prio = __rt_effective_prio(pi_task, p->normal_prio);

	/*
	 * If nothing changed; bail early.
	 */
	if (p->pi_top_task == pi_task && prio == p->prio && !dl_prio(prio))
		return;

	rq = __task_rq_lock(p, &rf);
	update_rq_clock(rq);
        // ...
	p->pi_top_task = pi_task;

	/*
	 * For FIFO/RR we only need to set prio, if that matches we're done.
	 */
	if (prio == p->prio && !dl_prio(prio))
		goto out_unlock;
        // ...
	if (unlikely(p == rq->idle)) {
		WARN_ON(p != rq->curr);
		WARN_ON(p->pi_blocked_on);
		goto out_unlock;
	}

	trace_sched_pi_setprio(p, pi_task); // Tracepoint
	oldprio = p->prio;

	if (oldprio == prio)
		queue_flag &= ~DEQUEUE_MOVE;

	prev_class = p->sched_class;
	queued = task_on_rq_queued(p);
	running = task_current(rq, p);
	if (queued)
		dequeue_task(rq, p, queue_flag);
	if (running)
		put_prev_task(rq, p);
        // ...
}
\end{code}

\paragraph{\texttt{trace\_sched\_move\_numa}}
Tracks migration of tasks from one CPU to another (basically to a different runqueue). Can be used to detect if automatic NUMA balancing is bouncing between nodes.\\The event is almost identical to \texttt{trace\_sched\_swap\_numa}, but it's a single task being migrated.

\textbf{Parameters}
\begin{itemize}
    \item \verb|struct task_struct *tsk| -- Migrated task
    \item \verb|int src_cpu|
    \item \verb|int dst_cpu|
\end{itemize}

\textbf{Trace output (\texttt{sched\_move\_task\_template})}
\begin{itemize}
    \item \verb|pid_t pid, tgid, ngid| -- pid, thread group id, and NUMA group id
    \item \verb|int src_cpu, src_nid| -- nid is the NUMA node id of the cpu \verb|src_cpu|
    \item \verb|int dst_cpu, dst_nid|
\end{itemize}

\textbf{Usages}
\begin{code}
// ./kernel/sched/core.c:5485
/* Migrate current task p to target_cpu */
int migrate_task_to(struct task_struct *p, int target_cpu) {
	struct migration_arg arg = { p, target_cpu };
	int curr_cpu = task_cpu(p);

	if (curr_cpu == target_cpu)
		return 0;

	if (!cpumask_test_cpu(target_cpu, &p->cpus_allowed))
		return -EINVAL;

	/* TODO: This is not properly updating schedstats */

	trace_sched_move_numa(p, curr_cpu, target_cpu);
	return stop_one_cpu(curr_cpu, migration_cpu_stop, &arg);
}
\end{code}
The subroutine \verb|stop_one_cpu()| stops the target CPU and runs \verb|migration_cpu_stop| with arguments \verb|arg| on the CPU, returns when the execution is complete. It essentially works like \verb|stop_two_cpus()|.

\section{Events in \texttt{fair.c}}
\paragraph{\texttt{trace\_sched\_stick\_numa}}
Triggered when a NUMA migration fails. 

\textbf{Parameters}
\begin{itemize}
    \item \verb|struct task_struct *tsk| -- (not) Migrated task
    \item \verb|int src_cpu|
    \item \verb|int dst_cpu|
\end{itemize}

\textbf{Trace output (\texttt{sched\_move\_task\_template})}
\begin{itemize}
    \item \verb|pid_t pid, tgid, ngid| -- pid, thread group id, and NUMA group id
    \item \verb|int src_cpu, src_nid| -- nid is the NUMA node id of the cpu \verb|src_cpu|
    \item \verb|int dst_cpu, dst_nid|
\end{itemize}

\textbf{Usages}
\begin{code}
// ./kernel/sched/fair.c:1836/1844
static int task_numa_migrate(struct task_struct *p)
{
	struct task_numa_env env = {
		.p = p,

		.src_cpu = task_cpu(p),
		.src_nid = task_node(p),

		.imbalance_pct = 112,

		.best_task = NULL,
		.best_imp = 0,
		.best_cpu = -1,
	};
	struct sched_domain *sd;
	struct rq *best_rq;
	unsigned long taskweight, groupweight;
	int nid, ret, dist;
	long taskimp, groupimp;
        // ...
	/*
	 * Cpusets can break the scheduler domain tree into smaller
	 * balance domains, some of which do not cross NUMA boundaries.
	 * Tasks that are "trapped" in such domains cannot be migrated
	 * elsewhere, so there is no point in (re)trying.
	 */
	if (unlikely(!sd)) {
		sched_setnuma(p, task_node(p));
		return -EINVAL;
	}

	env.dst_nid = p->numa_preferred_nid;
	dist = env.dist = node_distance(env.src_nid, env.dst_nid);
	taskweight = task_weight(p, env.src_nid, dist);
	groupweight = group_weight(p, env.src_nid, dist);
	update_numa_stats(&env.src_stats, env.src_nid);
	taskimp = task_weight(p, env.dst_nid, dist) - taskweight;
	groupimp = group_weight(p, env.dst_nid, dist) - groupweight;
	update_numa_stats(&env.dst_stats, env.dst_nid);

	task_numa_find_cpu(&env, taskimp, groupimp);
        // ... looks for NUMA nodes ...
	/* No better CPU than the current one was found. */
	if (env.best_cpu == -1)
		return -EAGAIN;

	best_rq = cpu_rq(env.best_cpu);
	if (env.best_task == NULL) {
		ret = migrate_task_to(p, env.best_cpu);
		WRITE_ONCE(best_rq->numa_migrate_on, 0);
		if (ret != 0)
			trace_sched_stick_numa(p, env.src_cpu, env.best_cpu); // Tracepoint
		return ret;
	}

	ret = migrate_swap(p, env.best_task, env.best_cpu, env.src_cpu);
	WRITE_ONCE(best_rq->numa_migrate_on, 0);

	if (ret != 0)
		trace_sched_stick_numa(p, env.src_cpu, task_cpu(env.best_task)); // Tracepoint
	put_task_struct(env.best_task);
	return ret;
}
\end{code}
This function tries to migrate the task to a better CPU, if it fails, the event is triggered. The actual migration happens in \verb|migrate_task_to()|.

\paragraph{\texttt{trace\_sched\_stat\_runtime}}
Tracepoint for accounting runtime and vruntime.

\textbf{Parameters}
\begin{itemize}
    \item \verb|struct task_struct *tsk| -- Task with updated stats
    \item \verb|u64 runtime| -- Updated true runtime
    \item \verb|u64 vruntime| -- Updated virtual runtime
\end{itemize}

\textbf{Trace output}
\begin{itemize}
    \item \verb|char *comm|
    \item \verb|pid_t pid|
    \item \verb|u64 runtime|
    \item \verb|u64 vruntime|
\end{itemize}

\textbf{Usages}\\
The tracepoint is located at the end of the \verb|update_curr()| function, shown in Section \ref{trace:sched_stat_runtime}. The event shows all the relevant updated statistics for the process that just got updated after a timer interrupt. This event was also used as an example for tracing in Chapter \ref{chap:ftrace}.

\paragraph{\texttt{trace\_sched\_stat\_wait}}
Tracepoint for accounting wait time. That is, the time the task is runnable but not actually running due to scheduler contention.

\textbf{Parameters}
\begin{itemize}
    \item \verb|struct task_struct *tsk| -- Task with updated stats
    \item \verb|u64 delay| -- Wait time
\end{itemize}

\textbf{Trace output (\texttt{sched\_stat\_template})}
\begin{itemize}
    \item \verb|char *comm|
    \item \verb|pid_t pid|
    \item \verb|u64 delay|
\end{itemize}

\textbf{Usages}
\begin{code}
// ./kernel/sched/fair.c:886
static inline void
update_stats_wait_end(struct cfs_rq *cfs_rq, struct sched_entity *se){
	struct task_struct *p;
	u64 delta;

	if (!schedstat_enabled())
		return;

	delta = rq_clock(rq_of(cfs_rq)) - schedstat_val(se->statistics.wait_start);

	if (entity_is_task(se)) {
		p = task_of(se);
		if (task_on_rq_migrating(p)) {
			/*
			 * Preserve migrating task's wait time so wait_start
			 * time stamp can be adjusted to accumulate wait time
			 * prior to migration.
			 */
			__schedstat_set(se->statistics.wait_start, delta);
			return;
		}
		trace_sched_stat_wait(p, delta); // Tracepoint
	}

	__schedstat_set(se->statistics.wait_max,
		      max(schedstat_val(se->statistics.wait_max), delta));
	__schedstat_inc(se->statistics.wait_count);
	__schedstat_add(se->statistics.wait_sum, delta);
	__schedstat_set(se->statistics.wait_start, 0);
}
\end{code}

\paragraph{\texttt{trace\_sched\_stat\_*}(\texttt{sleep/iowait/blocked})} 
The purpose of these three events is very similar: account updated statistics, like \texttt{trace\_sched\_stat\_runtime} and \texttt{trace\_sched\_stat\_wait}.
They also have the same parameters and output format (same output template).
\begin{itemize}
    \item \texttt{trace\_sched\_stat\_sleep} -- Tracepoint for accounting sleep time: the time the task is not runnable, including iowait.
    \item \texttt{trace\_sched\_stat\_iowait} -- Tracepoint for accounting iowait time: the time the task is not runnable due to waiting on IO to complete.
    \item \texttt{trace\_sched\_stat\_blocked} -- Tracepoint for accounting blocked time: time the task is in \verb|TASK_UNINTERRUPTIBLE|.
\end{itemize}

\textbf{Parameters}
\begin{itemize}
    \item \verb|struct task_struct *tsk| -- Task with updated stats
    \item \verb|u64 delay| -- Sleep/iowait/blocked time
\end{itemize}

\textbf{Trace output (\texttt{sched\_stat\_template})}
\begin{itemize}
    \item \verb|char *comm|
    \item \verb|pid_t pid|
    \item \verb|u64 delay|
\end{itemize}

\textbf{Usages}\\
\begin{code}
// ./kernel/sched/fair.c:925/944/947
static inline void
update_stats_enqueue_sleeper(struct cfs_rq *cfs_rq, struct sched_entity *se) {
	struct task_struct *tsk = NULL;
	u64 sleep_start, block_start;

	if (!schedstat_enabled())
		return;

	sleep_start = schedstat_val(se->statistics.sleep_start);
	block_start = schedstat_val(se->statistics.block_start);

	if (entity_is_task(se))
		tsk = task_of(se);

	if (sleep_start) {
		u64 delta = rq_clock(rq_of(cfs_rq)) - sleep_start;

		if ((s64)delta < 0)
			delta = 0;

		if (unlikely(delta > schedstat_val(se->statistics.sleep_max)))
			__schedstat_set(se->statistics.sleep_max, delta);

		__schedstat_set(se->statistics.sleep_start, 0);
		__schedstat_add(se->statistics.sum_sleep_runtime, delta);

		if (tsk) {
			account_scheduler_latency(tsk, delta >> 10, 1);
			trace_sched_stat_sleep(tsk, delta); // Tracepoint
		}
	}
	if (block_start) {
		u64 delta = rq_clock(rq_of(cfs_rq)) - block_start;

		if ((s64)delta < 0)
			delta = 0;

		if (unlikely(delta > schedstat_val(se->statistics.block_max)))
			__schedstat_set(se->statistics.block_max, delta);

		__schedstat_set(se->statistics.block_start, 0);
		__schedstat_add(se->statistics.sum_sleep_runtime, delta);

		if (tsk) {
			if (tsk->in_iowait) {
				__schedstat_add(se->statistics.iowait_sum, delta);
				__schedstat_inc(se->statistics.iowait_count);
				trace_sched_stat_iowait(tsk, delta); // Tracepoint
			}

			trace_sched_stat_blocked(tsk, delta); // Tracepoint

			/*
			 * Blocking time is in units of nanosecs, so shift by
			 * 20 to get a milliseconds-range estimation of the
			 * amount of time that the task spent sleeping:
			 */
			if (unlikely(prof_on == SLEEP_PROFILING)) {
				profile_hits(SLEEP_PROFILING,
						(void *)get_wchan(tsk),
						delta >> 20);
			}
			account_scheduler_latency(tsk, delta >> 10, 0);
		}
	}
}
\end{code}
Their tracepoints are all located in the same function. Its purpouse is to do accounting for scheduling statistics. It is executed when a sleeping task is woken up upon insertion in the runqueue, to account how much time it waited for it to happen. The task could have been sleeping because of iowait or being blocked: the function treats them as separate cases. \verb|update_stats_enqueue_sleeper()| is called from \verb|update_stats_enqueue()|:
\begin{code}
/*
 * Task is being enqueued - update stats:
 */
static inline void
update_stats_enqueue(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags) {
	if (!schedstat_enabled())
		return;
	/*
	 * Are we enqueueing a waiting task? (for current tasks
	 * a dequeue/enqueue event is a NOP)
	 */
	if (se != cfs_rq->curr)
		update_stats_wait_start(cfs_rq, se);

	if (flags & ENQUEUE_WAKEUP)
		update_stats_enqueue_sleeper(cfs_rq, se);
}
\end{code}

\section{Events in other source files}
\paragraph{\texttt{trace\_sched\_process\_exec}}
Tracepoint for \verb|exec()|. The function replaces the stack and process image with the new one, then assigns a new pid. This function is used in syscall handlers such as \verb|__do_execve_file()|. 

\textbf{Parameters}
\begin{itemize}
    \item \verb|struct task_struct *p| -- New process image, which will replace the current one
    \item \verb|pid_t old_pid| -- Replaced process's pid (\verb|current->pid|)
    \item \verb|struct linux_binprm *bprm| -- This structure is used to hold the arguments that are used when loading binaries (\verb|brp| = BinaRy Parameter).
\end{itemize}

\textbf{Trace output}
\begin{itemize}
    \item \verb|char *filename| -- The loaded binary's name (\verb|bprm->filename|)
    \item \verb|pid_t pid| -- New pid (simply \verb|p->pid|)
    \item \verb|pid_t old_pid|
\end{itemize}

\textbf{Usages}
\begin{code}
// ./fs/exec.c:1698
static int exec_binprm(struct linux_binprm *bprm){
	pid_t old_pid, old_vpid;
	int ret;

	/* Need to fetch pid before load_binary changes it */
	old_pid = current->pid;
	rcu_read_lock();
	old_vpid = task_pid_nr_ns(current, task_active_pid_ns(current->parent));
	rcu_read_unlock();

	ret = search_binary_handler(bprm);
	if (ret >= 0) {
		audit_bprm(bprm);
		trace_sched_process_exec(current, old_pid, bprm); // Tracepoint
		ptrace_event(PTRACE_EVENT_EXEC, old_vpid);
		proc_exec_connector(current);
	}

	return ret;
}
\end{code}
  
\paragraph{\texttt{trace\_sched\_process\_wait}}
Tracepoint for a waiting task. Located in the \verb|wait()| system call handler.

\textbf{Parameters}
\begin{itemize}
    \item \verb|struct pid *pid| -- Waiting task
\end{itemize}

\textbf{Trace output}
\begin{itemize}
    \item \verb|char *comm|
    \item \verb|pid_t pid|
    \item \verb|int prio|
\end{itemize}

\textbf{Usages}
\begin{code}
// ./kernel/exit.c:1503
static long do_wait(struct wait_opts *wo){
	struct task_struct *tsk;
	int retval;

	trace_sched_process_wait(wo->wo_pid); // Tracepoint

	init_waitqueue_func_entry(&wo->child_wait, child_wait_callback);
	wo->child_wait.private = current;
	add_wait_queue(&current->signal->wait_chldexit, &wo->child_wait);
repeat:
	/*
	 * If there is nothing that can match our criteria, just get out.
	 * We will clear ->notask_error to zero if we see any child that
	 * might later match our criteria, even if we are not able to reap
	 * it yet.
	 */
	wo->notask_error = -ECHILD;
	if ((wo->wo_type < PIDTYPE_MAX) &&
	   (!wo->wo_pid || hlist_empty(&wo->wo_pid->tasks[wo->wo_type])))
		goto notask;

	set_current_state(TASK_INTERRUPTIBLE);
	read_lock(&tasklist_lock);
	tsk = current;
	do {
		retval = do_wait_thread(wo, tsk);
		if (retval)
			goto end;

		retval = ptrace_do_wait(wo, tsk);
		if (retval)
			goto end;

		if (wo->wo_flags & __WNOTHREAD)
			break;
	} while_each_thread(current, tsk);
	read_unlock(&tasklist_lock);

notask:
	retval = wo->notask_error;
	if (!retval && !(wo->wo_flags & WNOHANG)) {
		retval = -ERESTARTSYS;
		if (!signal_pending(current)) {
			schedule();
			goto repeat;
		}
	}
end:
	__set_current_state(TASK_RUNNING);
	remove_wait_queue(&current->signal->wait_chldexit, &wo->child_wait);
	return retval;
}
\end{code}

\paragraph{\texttt{trace\_sched\_process\_free}}
Tracepoint for freeing a task.

\textbf{Parameters}
\begin{itemize}
    \item \verb|struct task_struct *p| -- Task to be freed
\end{itemize}

\textbf{Trace output (\texttt{sched\_process\_template})}
\begin{itemize}
    \item \verb|char *comm|
    \item \verb|pid_t pid|
    \item \verb|int prio|
\end{itemize}

\textbf{Usages}
\begin{code}
// ./kernel/exit.c:180
static void delayed_put_task_struct(struct rcu_head *rhp){
	struct task_struct *tsk = container_of(rhp, struct task_struct, rcu);

	perf_event_delayed_put(tsk);
	trace_sched_process_free(tsk); // Tracepoint
	put_task_struct(tsk);
}
\end{code}
Deallocates \verb|task_struct|.
  
\paragraph{\texttt{trace\_sched\_process\_exit}}
Tracepoint for a task exiting. Located in the \verb|exit()| system call handler.

\textbf{Parameters}
\begin{itemize}
    \item \verb|struct task_struct *p| -- Exiting task
\end{itemize}

\textbf{Trace output (\texttt{sched\_process\_template})}
\begin{itemize}
    \item \verb|char *comm|
    \item \verb|pid_t pid|
    \item \verb|int prio|
\end{itemize}

\textbf{Usages} 
\begin{code}
// ./kernel/exit.c:866
void __noreturn do_exit(long code){
	struct task_struct *tsk = current;
	int group_dead;

	profile_task_exit(tsk);
	kcov_task_exit(tsk);

	WARN_ON(blk_needs_flush_plug(tsk));

	if (unlikely(in_interrupt()))
		panic("Aiee, killing interrupt handler!"); // Kernel panic
	if (unlikely(!tsk->pid))
		panic("Attempted to kill the idle task!"); // Kernel panic

	/*
	 * If do_exit is called because this processes oopsed, it's possible
	 * that get_fs() was left as KERNEL_DS, so reset it to USER_DS before
	 * continuing. Amongst other possible reasons, this is to prevent
	 * mm_release()->clear_child_tid() from writing to a user-controlled
	 * kernel address.
	 */
	set_fs(USER_DS);

	ptrace_event(PTRACE_EVENT_EXIT, code);

	validate_creds_for_do_exit(tsk);

	/*
	 * We're taking recursive faults here in do_exit. Safest is to just
	 * leave this task alone and wait for reboot.
	 */
	if (unlikely(tsk->flags & PF_EXITING)) {
		pr_alert("Fixing recursive fault but reboot is needed!\n");
        // ...
		tsk->flags |= PF_EXITPIDONE;
		set_current_state(TASK_UNINTERRUPTIBLE);
		schedule();
	}

	exit_signals(tsk);  /* sets PF_EXITING */
        // ...
	tsk->exit_code = code;
	taskstats_exit(tsk, group_dead);

	exit_mm();

	if (group_dead)
		acct_process();
	trace_sched_process_exit(tsk); // Tracepoint

	exit_sem(tsk);
	exit_shm(tsk);
	exit_files(tsk);
	exit_fs(tsk);
	if (group_dead)
		disassociate_ctty(1);
	exit_task_namespaces(tsk);
	exit_task_work(tsk);
	exit_thread(tsk);

	/*
	 * Flush inherited counters to the parent - before the parent
	 * gets woken up by child-exit notifications.
	 *
	 * because of cgroup mode, must be called before cgroup_exit()
	 */
	perf_event_exit_task(tsk);

	sched_autogroup_exit_task(tsk);
	cgroup_exit(tsk);

	/*
	 * FIXME: do that only when needed, using sched_exit tracepoint
	 */
	flush_ptrace_hw_breakpoint(tsk);
        // ...
	lockdep_free_task(tsk);
	do_task_dead();
}
\end{code}
  
\paragraph{\texttt{trace\_sched\_process\_fork}}
Tracepoint for \verb|fork()|. Located in the \verb|fork()| system call handler.

\textbf{Parameters}
\begin{itemize}
    \item \verb|struct task_struct *parent| -- Parent task
    \item \verb|struct task_struct *child| -- Newly spawned child task
\end{itemize}

\textbf{Trace output}
\begin{itemize}
    \item \verb|char *parent_comm|
    \item \verb|pid_t parent_pid|
    \item \verb|char *child_comm|
    \item \verb|pid_t child_pid|
\end{itemize}

\textbf{Usages}
\begin{code}
// ./kernel/fork.c:2242
/*
 *  Ok, this is the main fork-routine.
 *
 * It copies the process, and if successful kick-starts
 * it and waits for it to finish using the VM if required.
 */
long _do_fork(unsigned long clone_flags,
	      unsigned long stack_start,
	      unsigned long stack_size,
	      int __user *parent_tidptr,
	      int __user *child_tidptr,
	      unsigned long tls){
	struct completion vfork;
	struct pid *pid;
	struct task_struct *p;
	int trace = 0;
	long nr;

	/*
	 * Determine whether and which event to report to ptracer.  When
	 * called from kernel_thread or CLONE_UNTRACED is explicitly
	 * requested, no event is reported; otherwise, report if the event
	 * for the type of forking is enabled.
	 */
	if (!(clone_flags & CLONE_UNTRACED)) {
		if (clone_flags & CLONE_VFORK)
			trace = PTRACE_EVENT_VFORK;
		else if ((clone_flags & CSIGNAL) != SIGCHLD)
			trace = PTRACE_EVENT_CLONE;
		else
			trace = PTRACE_EVENT_FORK;

		if (likely(!ptrace_event_enabled(current, trace)))
			trace = 0;
	}

	p = copy_process(clone_flags, stack_start, stack_size,
			 child_tidptr, NULL, trace, tls, NUMA_NO_NODE);
	add_latent_entropy();

	if (IS_ERR(p))
		return PTR_ERR(p);

	/*
	 * Do this prior waking up the new thread - the thread pointer
	 * might get invalid after that point, if the thread exits quickly.
	 */
	trace_sched_process_fork(current, p); // Tracepoint

	pid = get_task_pid(p, PIDTYPE_PID);
	nr = pid_vnr(pid);

	if (clone_flags & CLONE_PARENT_SETTID)
		put_user(nr, parent_tidptr);

	if (clone_flags & CLONE_VFORK) {
		p->vfork_done = &vfork;
		init_completion(&vfork);
		get_task_struct(p);
	}

	wake_up_new_task(p);

	/* forking complete and child started to run, tell ptracer */
	if (unlikely(trace))
		ptrace_event_pid(trace, pid);

	if (clone_flags & CLONE_VFORK) {
		if (!wait_for_vfork_done(p, &vfork))
			ptrace_event_pid(PTRACE_EVENT_VFORK_DONE, pid);
	}

	put_pid(pid);
	return nr;
}
\end{code}
\verb|clone_flags| is the \verb|flags| parameter of the \verb|clone()| system call. With this parameter it's possible to spawn a thread or an entire new process. This event was mentioned in Section \ref{trace:sched_process_fork}.
    
\paragraph{\texttt{trace\_sched\_process\_hang}}
Detect hung task.

\textbf{Parameters}
\begin{itemize}
    \item \verb|struct task_struct *tsk| -- Hung task
\end{itemize}

\textbf{Trace output}
\begin{itemize}
    \item \verb|char *comm|
    \item \verb|pid_t pid|
\end{itemize}

\textbf{Usages}
\begin{code}
// ./kernel/hung_task.c:113
static void check_hung_task(struct task_struct *t, unsigned long timeout){
	unsigned long switch_count = t->nvcsw + t->nivcsw;

	/*
	 * Ensure the task is not frozen.
	 * Also, skip vfork and any other user process that freezer should skip.
	 */
	if (unlikely(t->flags & (PF_FROZEN | PF_FREEZER_SKIP)))
	    return;

	/*
	 * When a freshly created task is scheduled once, changes its state to
	 * TASK_UNINTERRUPTIBLE without having ever been switched out once, it
	 * musn't be checked.
	 */
	if (unlikely(!switch_count))
		return;

	if (switch_count != t->last_switch_count) {
		t->last_switch_count = switch_count;
		t->last_switch_time = jiffies;
		return;
	}
	if (time_is_after_jiffies(t->last_switch_time + timeout * HZ))
		return;

	trace_sched_process_hang(t); // Tracepoint

	if (sysctl_hung_task_panic) {
		console_verbose();
		hung_task_show_lock = true;
		hung_task_call_panic = true;
	}

	/*
	 * Ok, the task did not get scheduled for more than 2 minutes,
	 * complain:
	 */
	if (sysctl_hung_task_warnings) {
		if (sysctl_hung_task_warnings > 0)
			sysctl_hung_task_warnings--;
		pr_err("INFO: task %s:%d blocked for more than %ld seconds.\n",
			t->comm, t->pid, timeout);
		pr_err("      %s %s %.*s\n",
			print_tainted(), init_utsname()->release,
			(int)strcspn(init_utsname()->version, " "),
			init_utsname()->version);
		pr_err("\"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\""
			" disables this message.\n");
		sched_show_task(t);
		hung_task_show_lock = true;
	}

	touch_nmi_watchdog();
}
\end{code}
Kernel thread for detecting tasks stuck in D state
  
\paragraph{\texttt{trace\_sched\_kthread\_*}(\texttt{stop/stop\_ret})}
\begin{itemize}
    \item \texttt{trace\_sched\_kthread\_stop} -- Tracepoint for calling \verb|kthread_stop|, performed to end a kthread.
    \item \texttt{trace\_sched\_kthread\_stop\_ret} -- Tracepoint for the return value of the kthread stopping.
\end{itemize}

\textbf{Parameters}
\begin{itemize}
    \item \verb|struct task_struct *t| -- Stopped kernel thread
\end{itemize}

\textbf{Trace output}
\begin{itemize}
    \item \verb|char *comm|
    \item \verb|pid_t pid|
\end{itemize}

\texttt{trace\_sched\_kthread\_stop\_ret} has just the return value (\verb|ret|) as parameter and trace output. Both tracepoints are located in the same function.

\textbf{Usages}
\begin{code}
// ./kernel/kthread.c:543/554
/**
 * kthread_stop - stop a thread created by kthread_create().
 * @k: thread created by kthread_create().
 *
 * Sets kthread_should_stop() for @k to return true, wakes it, and
 * waits for it to exit. This can also be called after kthread_create()
 * instead of calling wake_up_process(): the thread will exit without
 * calling threadfn().
 *
 * If threadfn() may call do_exit() itself, the caller must ensure
 * task_struct can't go away.
 *
 * Returns the result of threadfn(), or %-EINTR if wake_up_process()
 * was never called.
 */
int kthread_stop(struct task_struct *k){
	struct kthread *kthread;
	int ret;

	trace_sched_kthread_stop(k); // Tracepoint

	get_task_struct(k);
	kthread = to_kthread(k);
	set_bit(KTHREAD_SHOULD_STOP, &kthread->flags);
	kthread_unpark(k);
	wake_up_process(k);
	wait_for_completion(&kthread->exited);
	ret = k->exit_code;
	put_task_struct(k);

	trace_sched_kthread_stop_ret(ret); // Tracepoint
	return ret;
}
\end{code}